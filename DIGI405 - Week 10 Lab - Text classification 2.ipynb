{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405 Lab Class 10: Text classification 2\n",
    "\n",
    "*Thanks to Phil Davies for his great work adding new functionality to this notebook and his contributions to last week's notebook.*\n",
    "\n",
    "What can text classification techniques tell us about sentiment or tone? Can text classification help us find distinguishing features between two groups of texts?  \n",
    "\n",
    "This notebook introduces you to:\n",
    "\n",
    "1. A new data-set relevant to sentiment classification.  \n",
    "2. Feature selection using [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "3. Automating parameter tuning using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "**Remember:** Each time you change settings below, you will need to rerun the cells that create the pipeline and does the classification.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task:</strong> Throughout the notebook there are defined tasks for you to do. Watch out for them - they will have a box around them like this! Make sure you take some notes as you go.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Below we are importing required libraries. We will be using [scikit-learn](https://scikit-learn.org) for text classification in DIGI405. We will use the Naive Bayes Classifier. Scikit-learn has different feature extraction methods based on counts or tf-idf weights. We will also use NLTK for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# The following added to the Lab9 code.\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np #note last week's lab used 'import numpy'\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to make sure you have the stop word list and other data for pre-processing.\n",
    "\n",
    "This cell will also download one of the data-sets that comes pre-packaged with NLTK: a data-set of movie reviews. There is more information on this below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads some defaults for the stop word lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = None\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some functions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice preview of document\n",
    "def get_preview(docs, targets, target_names, doc_id, max_len=0):\n",
    "    preview = ''\n",
    "    if max_len < 1:\n",
    "        preview += 'Label\\n'\n",
    "        preview += '=====\\n'\n",
    "    else:\n",
    "        preview += str(doc_id)\n",
    "        preview += '\\t'\n",
    "    if isinstance(targets[doc_id], str):\n",
    "        preview += targets[doc_id]\n",
    "    else:\n",
    "        preview += target_names[targets[doc_id]]\n",
    "    if max_len < 1:\n",
    "        preview += '\\n\\nFull Text\\n'\n",
    "        preview += '=========\\n'\n",
    "        preview += docs[doc_id]\n",
    "        preview += '\\n'\n",
    "    else:\n",
    "        excerpt = get_excerpt(docs[doc_id], max_len)\n",
    "        preview += '\\t' + excerpt\n",
    "    return preview\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "# generate an excerpt\n",
    "def get_excerpt(text, max_len):\n",
    "    excerpt = _RE_COMBINE_WHITESPACE.sub(' ',text[0:max_len])\n",
    "    if max_len < len(text):\n",
    "        excerpt += '...'\n",
    "    return excerpt.strip()\n",
    "\n",
    "# combine a defined stop word list (or no stop word list) with any extra stop words defined\n",
    "def set_stop_words(stop_word_list, extra_stop_words):\n",
    "    if len(extra_stop_words) > 0:\n",
    "        if stop_word_list is None:\n",
    "            stop_word_list = []\n",
    "        stop_words = list(stop_word_list) + extra_stop_words\n",
    "    else:\n",
    "        stop_words = stop_word_list\n",
    "        \n",
    "    return stop_words\n",
    "\n",
    "# initiate stemming or lemmatising\n",
    "def set_normaliser(normalise):\n",
    "    if normalise == 'PorterStemmer':\n",
    "        normaliser = PorterStemmer()\n",
    "    elif normalise == 'SnowballStemmer':\n",
    "        normaliser = SnowballStemmer('english')\n",
    "    elif normalise == 'WordNetLemmatizer':\n",
    "        normaliser = WordNetLemmatizer()\n",
    "    else:\n",
    "        normaliser = None\n",
    "    return normaliser\n",
    "\n",
    "# we are using a custom tokenisation process to allow different tokenisers and stemming/lemmatising ...\n",
    "def tokenise(doc):\n",
    "    global tokeniser, normalise, normaliser\n",
    "    \n",
    "    # you could obviously add more tokenisers here if you wanted ...\n",
    "    if tokeniser == 'sklearn':\n",
    "        tokenizer = RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b\") # this is copied straight from sklearn source\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "    elif tokeniser == 'word_tokenize':\n",
    "        tokens = word_tokenize(doc)\n",
    "    elif tokeniser == 'wordpunct':\n",
    "        tokens = wordpunct_tokenize(doc)\n",
    "    elif tokeniser == 'nopunct':\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "    else:\n",
    "        tokens = word_tokenize(doc)\n",
    "        \n",
    "    # if using a normaliser then iterate through tokens and return the normalised tokens ...\n",
    "    if normalise == 'PorterStemmer':\n",
    "        return [normaliser.stem(t) for t in tokens]\n",
    "    elif normalise == 'SnowballStemmer':\n",
    "        return [normaliser.stem(t) for t in tokens]\n",
    "    elif normalise == 'WordNetLemmatizer':\n",
    "        # NLTK's lemmatiser needs parts of speech, otherwise assumes everything is a noun\n",
    "        pos_tokens = nltk.pos_tag(tokens)\n",
    "        lemmatised_tokens = []\n",
    "        for token in pos_tokens:\n",
    "            # NLTK's lemmatiser needs specific values for pos tags - this rewrites them ...\n",
    "            # default to noun\n",
    "            tag = wordnet.NOUN\n",
    "            if token[1].startswith('J'):\n",
    "                tag = wordnet.ADJ\n",
    "            elif token[1].startswith('V'):\n",
    "                tag = wordnet.VERB\n",
    "            elif token[1].startswith('R'):\n",
    "                tag = wordnet.ADV\n",
    "            lemmatised_tokens.append(normaliser.lemmatize(token[0],tag))\n",
    "        return lemmatised_tokens\n",
    "    else:\n",
    "        # no normaliser so just return tokens\n",
    "        return tokens\n",
    "\n",
    "# CountVectorizer pre-processor - remove numerics.\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus and set train/test split\n",
    "\n",
    "Today we will work with a movies reviews data-set. The reviews are annotated with sentiment polarities \"pos\" and \"neg\". You can get more information on the data-set and download it from [NLTK's data-set page](http://www.nltk.org/nltk_data/) and the [movie reviews data page](https://www.cs.cornell.edu/people/pabo/movie-review-data/)).\n",
    "\n",
    "This cell also sets the train/test split. 80% of the data is used for training and 20% is used for testing. The documents are assigned to each group randomly. It can be useful to rerun this cell to reshuffle your dataset so you can evaluate your model using different data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of class names.\n",
    "dataset_target_names = ['neg', 'pos']\n",
    "\n",
    "# assign the train/test split - 0.2 is 80% for training, 20% for testing\n",
    "test_size = 0.2\n",
    "\n",
    "#\n",
    "dataset_data = [(movie_reviews.raw(fileid))\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "dataset_target = [category\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# do the train test split ...\n",
    "# docs_train and docs_test are the documents\n",
    "# y_train and y_test are the labels\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset_data, \n",
    "                                                          dataset_target, \n",
    "                                                          test_size = test_size, \n",
    "                                                          random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect documents and labels\n",
    "\n",
    "In the next cells we can look at the data we have imported. Firstly, we will preview the document labels and a brief excerpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_id in range(len(docs_train)):\n",
    "    print(get_preview(docs_train, y_train, dataset_target_names, train_id, max_len=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this cell to inspect a specific document and its label based on its index in the training set. Note: The indexes will change each time you import the data above because of the random train/test split.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 1:</strong> Inspect some off the documents in each class and think about the kinds of words that might be useful features in this text classification task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = 11\n",
    "print(get_preview(docs_train, y_train, dataset_target_names, train_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction / Pre-processing\n",
    "\n",
    "You can consult last week's notebook for more information on each setting below.\n",
    "\n",
    "On the first run through, just use these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer\n",
    "lowercase = True\n",
    "tokeniser = 'nopunct' # note a new tokeniser setting has been added called nopunct based on tokenising with the regex \\w+\n",
    "normalise = None\n",
    "stop_word_list = nltk_stop_words\n",
    "extra_stop_words = []\n",
    "min_df = 0.0\n",
    "max_df = 1.0\n",
    "max_features = 1000\n",
    "ngram_range = (1, 1) \n",
    "encoding = 'utf-8'\n",
    "decode_error = 'ignore' # what to do if contains characters not of the given encoding - options 'strict', 'ignore', 'replace'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New settings for the feature selection step\n",
    "\n",
    "The feature selection step selects the top features based on [univariate statistical tests](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). Here we are using [mutual information scores](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) to assess the dependency between each feature and the class labels. \n",
    "\n",
    "The value below sets the number of features to select and use in our classifier. In this case we will start with 100 features, based on the mutual information score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a pipeline: feature extraction → feature selection → classifier\n",
    "\n",
    "**Important Note 1:** When you change settings above or reload your dataset you should rerun this cell!\n",
    "\n",
    "**Important Note 2:** This cell outputs the settings you used above, which you can cut and paste into a document to keep track of changes you are making and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "\n",
    "stop_words = set_stop_words(stop_word_list, extra_stop_words)\n",
    "normaliser = set_normaliser(normalise)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', Vectorizer(tokenizer = tokenise,\n",
    "                              lowercase = lowercase,\n",
    "                              min_df = min_df, \n",
    "                              max_df = max_df, \n",
    "                              max_features = max_features,\n",
    "                              stop_words = stop_words, \n",
    "                              ngram_range = ngram_range,\n",
    "                              encoding = encoding, \n",
    "                              preprocessor = preprocess_text,\n",
    "                              decode_error = decode_error)),\n",
    "    ('selector', SelectKBest(score_func = mutual_info_classif, k=kbest)),\n",
    "    ('classifier', MultinomialNB()), #here is where you would specify an alternative classifier\n",
    "])\n",
    "\n",
    "print('Classifier settings')\n",
    "print('===================')\n",
    "print('classifier:', type(pipeline.steps[2][1]).__name__)\n",
    "print('selector:', type(pipeline.steps[1][1]).__name__)\n",
    "print('vectorizer:', type(pipeline.steps[0][1]).__name__)\n",
    "print('classes:', dataset_target_names)\n",
    "print('lowercase:', lowercase)\n",
    "print('tokeniser:', tokeniser)\n",
    "print('normalise:', normalise)\n",
    "print('min_df:', min_df)\n",
    "print('max_df:', max_df)\n",
    "print('max_features:', max_features)\n",
    "if stop_word_list == nltk_stop_words:\n",
    "    print('stop_word_list:', 'nltk_stop_words')\n",
    "elif stop_word_list == sklearn_stop_words:\n",
    "    print('stop_word_list:', 'sklearn_stop_words')\n",
    "else:\n",
    "    print('stop_word_list:', 'None')\n",
    "print('extra_stop_words:', extra_stop_words)\n",
    "print('ngram_range:', ngram_range)\n",
    "print('encoding:', encoding)\n",
    "print('decode_error:', decode_error)\n",
    "print('kbest:', kbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier and predict labels on test data\n",
    "\n",
    "Because we are adding the feature selection step, the classifier will be slower as it has to calculate MI scores for each feature and rank them. This will increase the more features you extract.\n",
    "\n",
    "**Important Note:** You can cut and paste the model output into a document (with the settings above) to keep track of changes you are making and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "\n",
    "pipeline.fit(docs_train, y_train)\n",
    "y_predicted = pipeline.predict(docs_test)\n",
    "\n",
    "# print report\n",
    "print('Evaluation metrics')\n",
    "print('==================')\n",
    "print(metrics.classification_report(y_test, y_predicted, target_names = dataset_target_names))\n",
    "cm = metrics.confusion_matrix(y_true=y_test, y_pred=y_predicted, labels=dataset_target_names)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset_target_names)\n",
    "disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "vect = pipeline.steps[0][1]\n",
    "clf = pipeline.steps[2][1]\n",
    "\n",
    "logodds=clf.feature_log_prob_[1]-clf.feature_log_prob_[0]\n",
    "    \n",
    "lookup = dict((v,k) for k,v in vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at the features ranked by information gain (MI) and by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SelectKBest feature scores\n",
    "features = pipeline.named_steps['selector']\n",
    "# get top k feature indices\n",
    "cols = features.get_support(indices=True)\n",
    "# get corresponding feature scores \n",
    "top_k_feature_scores  = [features.scores_[i] for i in cols]\n",
    "\n",
    "# get vectorizer\n",
    "featnames = pipeline.named_steps['vectorizer']\n",
    "# get all feature names\n",
    "fred = featnames.get_feature_names()\n",
    "# get corresponding feature names\n",
    "top_k_feature_names = [fred[i] for i in cols]\n",
    "\n",
    "names_scores = list(zip(top_k_feature_names, top_k_feature_scores, logodds))\n",
    "ns_df = pd.DataFrame(data = names_scores, columns=['Feature_names', 'Feature_Scores', 'Log odds'])\n",
    "\n",
    "#Sort the dataframe for better visualization - change this to reorder by \n",
    "print('Top features by information gain')\n",
    "ns_df_sorted = ns_df.sort_values(['Feature_Scores', 'Log odds', 'Feature_names'], ascending = [False, False, False])\n",
    "display(ns_df_sorted[0:10])\n",
    "\n",
    "print('Features most indicative of', dataset_target_names[0])\n",
    "ns_df_sorted = ns_df.sort_values(['Log odds', 'Feature_Scores', 'Feature_names'], ascending = [True, False, False])\n",
    "display(ns_df_sorted[0:10])\n",
    "\n",
    "print('Features most indicative of', dataset_target_names[1])\n",
    "ns_df_sorted = ns_df.sort_values(['Log odds', 'Feature_Scores', 'Feature_names'], ascending = [False, False, False])\n",
    "display(ns_df_sorted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all features\n",
    "\n",
    "Just for your reference here is a count and list of all features extracted in the first step of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Total Features: ',len(vect.get_feature_names()))\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of all features selected based on information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Features: ',len(ns_df['Feature_names']))\n",
    "print(list(ns_df['Feature_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect correctly/incorrectly classified documents\n",
    "\n",
    "The wordclouds below only include the selected features that are used in the classification step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a counter for each cell in the confusion matrix\n",
    "counter = {}\n",
    "previews = {}\n",
    "for true_target, target_name in enumerate(dataset_target_names):\n",
    "    counter[target_name] = {}\n",
    "    previews[target_name] = {}\n",
    "    for predicted_target, ptarget_name in enumerate(dataset_target_names):\n",
    "        counter[target_name][ptarget_name] = {}\n",
    "        previews[target_name][ptarget_name] = ''\n",
    "\n",
    "# get doc-term matrix for test docs\n",
    "doc_terms = vect.transform(docs_test)\n",
    "\n",
    "# iterate through all predictions, building the counter and preview of docs\n",
    "# there is a better way to do this, but this will do!\n",
    "for doc_id, prediction in enumerate(pipeline.predict(docs_test)):\n",
    "    for k, v in enumerate(doc_terms[doc_id].toarray()[0]):\n",
    "        if v > 0 and lookup[k] in list(ns_df['Feature_names']):\n",
    "            if lookup[k] not in counter[y_test[doc_id]][prediction]:\n",
    "                counter[y_test[doc_id]][prediction][lookup[k]] = 0\n",
    "            counter[y_test[doc_id]][prediction][lookup[k]] += v\n",
    "    \n",
    "    previews[y_test[doc_id]][prediction] += get_preview(docs_test, y_test, dataset_target_names, doc_id, max_len=80) + '\\n'\n",
    "\n",
    "# output a wordcloud and preview of docs for each cell of confusion matrix ...\n",
    "for true_target, target_name in enumerate(dataset_target_names):\n",
    "    for predicted_target, ptarget_name in enumerate(dataset_target_names):\n",
    "        if true_target == predicted_target:\n",
    "            print(dataset_target_names[true_target],'Correctly classified')\n",
    "        else:\n",
    "            print(dataset_target_names[true_target],'incorrectly classified as',dataset_target_names[predicted_target])\n",
    "        print('=================================================================')\n",
    "\n",
    "        wordcloud = WordCloud(background_color=\"white\", width=800, height=600, color_func=lambda *args, **kwargs: \"black\").generate_from_frequencies(counter[target_name][ptarget_name])\n",
    "        plt.figure(figsize=(16, 8), dpi= 600)\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()        \n",
    "        \n",
    "        print(previews[target_name][ptarget_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 2:</strong> Inspect documents that were correct and incorrectly classified. Why are some documents incorrectly classified?\n",
    "</div>\n",
    "\n",
    "## Preview document and its features\n",
    "\n",
    "Use this cell to preview a document using its index in the test set. You can see the predicted label, its actual label, the full text and the features for this specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 4\n",
    "\n",
    "print('Prediction')\n",
    "print('==========')\n",
    "print(pipeline.predict([docs_test[test_id]])[0])\n",
    "print()\n",
    "\n",
    "print(get_preview(docs_test, y_test, dataset_target_names, test_id))\n",
    "\n",
    "print('Features')\n",
    "print('========')\n",
    "for k, v in enumerate(vect.transform([docs_test[test_id]]).toarray()[0]):\n",
    "    if v > 0 and lookup[k] in list(ns_df['Feature_names']):\n",
    "        print(v, '\\t', lookup[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 3:</strong> Try changing the tokenisation to include punctuation. What punctuation emerges as useful features? How are these punctuation features being used?\n",
    "</div>    \n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 4:</strong>    Now increase the number of most frequent tokens to allow the feature selection step to inspect and score lots more less frequent words. What number of frequent tokens improves the features that can be identified? \n",
    "</div>    \n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 5:</strong>    After you’ve identified the best settings to improve the performance metrics of the classifier, review the incorrectly classified documents again. Identify any unexpected word features and identify whether they may be true indicators of sentiment, or just coincidence.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Parameter Tuning\n",
    "\n",
    "In the remainder of the lab we will work through automated parameter tuning. Warning: this may take some time!\n",
    "\n",
    "## Create Search Space\n",
    "\n",
    "This step is to define the space of parameters and estimators we want to search through. We do this in the form of a dictionary and we use double underscore notation (__) to refer to the parameters of different steps in our pipeline. We will be trying out different values of k for the feature selector SelectKBest. However, this is not an exhaustive list of parameters that we can search. We could search parameters for the feature extraction step using the CountVectorizer or TfidfVectorizer as well. Some example parameters are commented out and you could test with them, but note that this can take considerable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline([\n",
    "    ('vectorizer', Vectorizer(tokenizer    = tokenise,\n",
    "                              lowercase    = lowercase,\n",
    "                              min_df       = min_df, \n",
    "                              max_df       = max_df,\n",
    "                              max_features = max_features,\n",
    "                              stop_words   = stop_words, \n",
    "                              ngram_range  = ngram_range,\n",
    "                              encoding     = encoding, \n",
    "                              preprocessor = preprocess_text,\n",
    "                              decode_error = decode_error)),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('classifier', MultinomialNB()), #here is where you would specify an alternative classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'selector__k'              : range(50, 701, 50), #this starts at 50 and ends at 700 with steps of 50\n",
    "                 #'selector__score_func'     : [mutual_info_classif, chi2],\n",
    "                 #'vectorizer'               : [CountVectorizer(), TfidfVectorizer()],\n",
    "                 #'vectorizer__max_features' : [700, 2000]\n",
    "                }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scorers can be either be one of the predefined metric strings or a scorer callable, like the one returned by make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'Accuracy': make_scorer(accuracy_score)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the GridSearch \n",
    "\n",
    "This is where the magic happens. We will now pass our pipeline into GridSearchCV to test our search space (of feature preprocessing, feature selection, model selection, and hyperparameter tuning combinations) using cross-validation with 3-folds. If we had more time, we would probably increase the number of folds (cv) to 5 or 10.\n",
    "\n",
    "Setting refit='Accuracy', refits an estimator on the whole dataset with the parameter setting that has the best cross-validated Accuracy score.\n",
    "That estimator is made available at ``clf2.best_estimator_`` along with parameters like ``clf2.best_score_``, ``clf2.best_params_`` and ``clf2.best_index_``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = GridSearchCV(estimator          = pipeline2, \n",
    "                    param_grid         = search_space, \n",
    "                    scoring            = scoring,\n",
    "                    cv                 = 3, \n",
    "                    refit              = 'Accuracy',\n",
    "                    return_train_score = True,\n",
    "                    verbose            = 1)\n",
    "clf2 = clf2.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Results\n",
    "\n",
    "We can access the best result of our search using the best_estimator_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting from the cv_results_ dictionary\n",
    "\n",
    "Demonstrating methods of extracting values from the cv_results_ dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = clf2.cv_results_['mean_test_Accuracy']\n",
    "stds  = clf2.cv_results_['std_test_Accuracy']\n",
    "\n",
    "for mean, std, params in zip(means, stds, clf2.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true, y_pred = y_test, clf2.best_estimator_.predict(docs_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the results\n",
    "\n",
    "Plot code taken from https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "\n",
    "Note: that the x and y axis range is set below - you may need to change this depending on values you chose above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our results\n",
    "results = clf2.cv_results_\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.title(\"GridSearchCV evaluating parameters using the Accuracy scorer.\",\n",
    "          fontsize=16)\n",
    "\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# adjust these according to your accuracy results and range values.\n",
    "ax.set_xlim(0, 700)\n",
    "ax.set_ylim(0.60, 1)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results['param_selector__k'].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(sorted(scoring), ['b']):\n",
    "    for sample, style in (('train', '--'), ('test', '-')):\n",
    "        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                        sample_score_mean + sample_score_std,\n",
    "                        alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "        ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                alpha=1 if sample == 'test' else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "    best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f with k=%s\" % (best_score, X_axis[best_index]),\n",
    "                (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got to this point in the lab, try changing search_space above to search more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
