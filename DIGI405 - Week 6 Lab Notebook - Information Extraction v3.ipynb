{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405 - Week 6 Lab Notebook - Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For advanced text processing capabilities, programming libraries like [Spacy](https://spacy.io/) offer a range of features. The aims of this notebook are to introduce Spacy and introduce information extraction using Spacy.  This will be the basis of your Lab work for week 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Spacy\n",
    "\n",
    "The Spacy library is already installed on JupyterHub. However, you may need to install a language model that Spacy will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get errors loading the spacy model, try running the cell below to install a language model, then re-run the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model cannot be located, try going to the Kernel menu and selecting 'Restart & Clear Output', then try loading the model again.\n",
    "\n",
    "An alternate path that may be useful if this is not working is:\n",
    "```\n",
    "nlp = spacy.load('/home/#####/.local/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-3.0.0')\n",
    "```\n",
    "Where ##### is your student userid (e.g. abc123)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Sometime between 1250 and 1300, Polynesians settled in the islands that later were named New Zealand and developed a distinctive Māori culture. \n",
    "In 1642, Dutch explorer Abel Tasman became the first European to sight New Zealand. \n",
    "In 1840, representatives of the United Kingdom and Māori chiefs signed the Treaty of Waitangi, which declared British sovereignty over the islands. \n",
    "In 1841, New Zealand became a colony within the British Empire and in 1907 it became a dominion; it gained full statutory independence in 1947 and the British monarch remained the head of state.\n",
    "'''\n",
    "# text from https://en.wikipedia.org/wiki/New_Zealand\n",
    "# you can replace the text variable with any text you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell tokenises and annotates some text with spacy and prints out the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise and annotate some text\n",
    "doc = nlp(text) \n",
    "\n",
    "# this will output the individual tokens \n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy can segment our text into sentences. This cell prints each sentence in turn ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech tagging with Spacy\n",
    "\n",
    "Here we are outputting a count by Penn Treebank tags. \n",
    "\n",
    "The [spacy.explain](https://spacy.io/api/top-level#spacy.explain) function can be used to output a user-friendly description for a given POS tag, dependency label or entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [token.tag_ for token in doc]\n",
    "\n",
    "tag_freq = Counter(tags)\n",
    "for tag in sorted(tag_freq, key=tag_freq.get, reverse=True):\n",
    "    print(tag, tag_freq[tag], spacy.explain(tag), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy's default part of speech tagging uses a simpler set of labels. Note: here the difference between the cell above and below (i.e. the Penn Treebank tags are accessed via tag_ and Spacy's POS tags are available via pos_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [token.pos_ for token in doc]\n",
    "\n",
    "tag_freq = Counter(tags)\n",
    "for tag in sorted(tag_freq, key=tag_freq.get, reverse=True):\n",
    "    print(tag, tag_freq[tag], spacy.explain(tag), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of filtering tokens by their part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'PROPN':\n",
    "        filtered_tokens.append(token)\n",
    "        \n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the tags listed above with their spacy.explain explanations. Replace 'PROPN' with other parts of speech, such as:\n",
    "\n",
    "- VERB\n",
    "- NOUN\n",
    "- ADP (preposition)\n",
    "- NUM (numbers)\n",
    "\n",
    "You can also use the more extensive Penn Treebank tag set under the \"English\" heading. This allows you to do things like differentiate between tenses of verbs (e.g. select verbs in past-tense, VBD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.tag_ == 'VBD': # similar to above, except instead of pos_ we use tag_ to access the different tag set\n",
    "        filtered_tokens.append(token)\n",
    "        \n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are outputting a frequency list for proper nouns. Change the code to output a frequency list for another part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select prop noun tokens only \n",
    "filtered_tokens = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
    "\n",
    "token_freq = Counter(filtered_tokens)\n",
    "for token in sorted(token_freq, key=token_freq.get, reverse=True):\n",
    "    print(token, token_freq[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this model to remove or normalise particular token types as required. For example, here we are normalising individual numbers to one token \"NUMBER\". This might be useful if you were interested in collocation patterns related to numbers in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_numbers = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NUM':\n",
    "        no_numbers.append('NUMBER') # if we wanted to remove numbers completely change this line to: continue\n",
    "    else:\n",
    "        no_numbers.append(token)\n",
    "\n",
    "print(no_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by character types of tokens\n",
    "\n",
    "You can filter by types of tokens. For example, here we are excluding any tokens with non-alphabetic characters such as numbers or '$'. Look at the list of token attributes here: https://spacy.io/api/token#attributes  \n",
    "Change is_alpha to another boolean (bool) type to filter in another way (e.g. is_digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_filtered = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_alpha is False:\n",
    "        continue\n",
    "    else:\n",
    "        char_filtered.append(token) \n",
    "\n",
    "print(char_filtered)\n",
    "# no dates or punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun phrases / chunks\n",
    "\n",
    "As we discussed in the lecture on information extraction, identifying noun chunks is a basic way of identifying entities within our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Spacy detects named entities. The labels for the entities for English-language models are documented here: https://spacy.io/models/en#en_core_web_sm. Expand out the \"Label Scheme\" section to see the various labels. \n",
    "\n",
    "You can use spacy.explain to find out more about a specific label. You can run the following cell with different labels for a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.explain('NORP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the named entities in our sample text and their frequency. Again, we are using spacy.explain here to give a user-friendly description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [ent.label_ for ent in doc.ents]\n",
    "\n",
    "entities_freq = Counter(entities)\n",
    "for entity in sorted(entities_freq, key=entities_freq.get, reverse=True):\n",
    "    print(entity, entities_freq[entity], spacy.explain(entity), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the entities listed out with their position within the text. \n",
    "\n",
    "Take a look at the results below and make sure you understand all the labels and the types of entities that Spacy can detect. \n",
    "\n",
    "How would you modify the named entity recognition code to only list Countries/Places (GPE)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.start_char, ent.end_char, ent.text,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has a \"pretty\" way to visualise named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing\n",
    "\n",
    "Dependency parsing analyses sentences besed on relationships between words.\n",
    "\n",
    "You can view the different annotation labels for Spacy's dependency parsing here: https://spacy.io/models/en#en_core_web_sm (expand the \"label scheme\" and see the PARSER labels). \n",
    "\n",
    "Spacy is packaged with a useful dependency visualiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of sentences\n",
    "sentences = list(doc.sents) # create a list of sentences\n",
    "# the sentences list can be passed to the following line, but here just displaying the shortest sentence\n",
    "spacy.displacy.render(sentences[1], style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task\n",
    "\n",
    "Select a short piece of text from the web (e.g. a news article, a blog post, a short story, a report) containing several names of people, places, organisations or other entities. Copy and paste it into the top of this notebook between the triple quotes to replace the ```text``` variable. Then run each cell again to extract part of speech, noun chunk and named entities. Spend some time investigating characteristics of the text using parts of speech, noun phrases, named entities, and dependendency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
