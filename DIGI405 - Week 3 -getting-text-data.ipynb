{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405 Week 3 - Getting text data: Example corpus-building using an API\n",
    "\n",
    "For your corpus building project you can collect data using an API. This notebook is intended to collect some data using [Wikipedia's API](https://www.mediawiki.org/wiki/API:Main_page), to introduce the way Jupyter mixes code and text, and to show you that with some simple edits we can collect some new data.\n",
    "\n",
    "Wikipedia articles are often used for language modelling. However, it should be obvious that Wikipedia represents a specific form of discourse that attempts to be factual. Wikipedia articles do not provide a good source for corpus-assisted discourse analysis where we are often interested in different points of view. What is interesting about Wikipedia is that there are \"Talk\" pages where Wikipedia editors can discuss specific pages. \n",
    "\n",
    "**Take a quick look at the [Talk:COVID-19](https://en.wikipedia.org/wiki/Talk:COVID-19) pages. Note: there are also archived conversations.**\n",
    "\n",
    "These are public pages for Wikipedia editors/users to discuss changes to Wikipedia articles.  \n",
    "\n",
    "These pages can be studied to understand how Wikipedia articles are produced and specific points of uncertainty, debate, disagreement or controversy about knowledge production.\n",
    "\n",
    "**Read through the text and comments below. Run each of the cells in turn.**\n",
    "\n",
    "## Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required code libraries\n",
    "# requests literally allows us to do web requests\n",
    "import requests \n",
    "\n",
    "# BeautifulSoup is used here to convert HTML to text (more on its use in scraping next lab)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# time library allows us to pause between requests below\n",
    "import time\n",
    "\n",
    "# library to do things with the operating system - in case with the file system\n",
    "import os\n",
    "\n",
    "# library used to zip your corpus\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings (you can change these!)\n",
    "\n",
    "This cell allows you to change the wiki page to collect, and other settings related to data collection with the wiki.\n",
    "\n",
    "Perhaps most importantly you can change the Wikipedia Talk pages to collect and where you are saving your corpus. \n",
    "\n",
    "Leave settings as they are the first time you run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the page slug for the wiki page you are interested in \n",
    "# e.g. for wiki talk pages about https://en.wikipedia.org/wiki/COVID-19, page var should be 'COVID-19'\n",
    "page = 'COVID-19'\n",
    "\n",
    "# directory to save corpus - if doesn't exist will be created by next cell\n",
    "corpus_path = 'talk-corpus/'\n",
    "\n",
    "# set api path - you could change this to another language wiki\n",
    "api_url = 'https://en.wikipedia.org/w/api.php'\n",
    "\n",
    "# set headers for our requests so that wikipedia knows where we are coming from as per their policy here:\n",
    "# https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "# my email address is in there as contact so don't abuse their API!\n",
    "headers = {'user-agent': 'DIGI405 Class Exercise/0.1-dev (https://www.canterbury.ac.nz/courseinfo/GetCourseDetails.aspx?course=DIGI405&occurrence=21S2(C)&year=2021)'}\n",
    "\n",
    "# seconds to pause between requests - set this to conservative number\n",
    "# don't set this to zero unless you want the class banned from wikipedia\n",
    "sleep_seconds = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates the directory to save your corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if corpus path doesn't exist create it\n",
    "if not os.path.exists(corpus_path):\n",
    "    os.makedirs(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wikipedia HTML output to text - note more cleanup could be done here\n",
    "# ideally wikipedia would output text from their API, they only output HTML or wikitext\n",
    "# both need to be cleaned up to get text.\n",
    "def wiki_html_to_txt(html):\n",
    "    # this uses BeautifulSoup - we will work with this next week\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    # cleanup the markup a little (removing tags not related to text)\n",
    "    # more cleanup is possible with the Wiki markup \n",
    "    for s in soup.select('.mw-references-wrap'):\n",
    "        s.extract()\n",
    "    for s in soup.select('sup.reference'):\n",
    "        s.extract()    \n",
    "    for s in soup.select('style'):\n",
    "        s.extract()    \n",
    "    for s in soup.select('.mw-editsection'):\n",
    "        s.extract()    \n",
    "    for s in soup.select('#toc'):\n",
    "        s.extract()    \n",
    "    for s in soup.select('.tmbox'):\n",
    "        s.extract()    \n",
    "\n",
    "    return soup.get_text().strip()\n",
    "    \n",
    "# rough and ready function to output readable filesnames with filesystem safe characters\n",
    "def url_to_filename(url):\n",
    "    url = url.replace('https://', '').replace('http://', '')\n",
    "    safe = []\n",
    "    for x in url:\n",
    "        if x.isalnum():\n",
    "            safe.append(x)\n",
    "        else:\n",
    "            safe.append('-')\n",
    "    filename = \"\".join(safe)\n",
    "\n",
    "    if len(filename) > 100: #prevent filenames over 200 - note this could create a conflict of filenames so check\n",
    "        filename = filename[:50] + '___' + filename[-50:]\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect a corpus of Wikipedia Talk pages\n",
    "\n",
    "Running the next cell will do some API calls, retrieve Wikipedia's HTML markup, convert this to text and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting https://en.wikipedia.org/w/api.php?action=query&list=allpages&apnamespace=1&apprefix=COVID-19/Archive&aplimit=100\n",
      "Click link above for human-readable view of json\n",
      "Add page: Talk:COVID-19/Archive 1\n",
      "Add page: Talk:COVID-19/Archive 10\n",
      "Add page: Talk:COVID-19/Archive 11\n",
      "Add page: Talk:COVID-19/Archive 12\n",
      "Add page: Talk:COVID-19/Archive 13\n",
      "Add page: Talk:COVID-19/Archive 14\n",
      "Add page: Talk:COVID-19/Archive 15\n",
      "Add page: Talk:COVID-19/Archive 16\n",
      "Add page: Talk:COVID-19/Archive 17\n",
      "Add page: Talk:COVID-19/Archive 18\n",
      "Add page: Talk:COVID-19/Archive 19\n",
      "Add page: Talk:COVID-19/Archive 2\n",
      "Add page: Talk:COVID-19/Archive 3\n",
      "Add page: Talk:COVID-19/Archive 4\n",
      "Add page: Talk:COVID-19/Archive 5\n",
      "Add page: Talk:COVID-19/Archive 6\n",
      "Add page: Talk:COVID-19/Archive 7\n",
      "Add page: Talk:COVID-19/Archive 8\n",
      "Add page: Talk:COVID-19/Archive 9\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_1\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_10\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_11\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_12\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_13\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_14\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_15\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_16\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_17\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_18\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_19\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_2\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_3\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_4\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_5\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_6\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_7\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_8\n",
      "Requesting https://en.wikipedia.org/w/api.php?action=parse&prop=text&page=Talk:COVID-19/Archive_9\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "# initiate a queue of Talk pages related to the page\n",
    "talk_pages = ['Talk:' + page]\n",
    "\n",
    "# build URL for request to Wikipedia API to retrieve archived talk pages\n",
    "# Documentation of API call: https://en.wikipedia.org/w/api.php?action=help&modules=query%2Ballpages\n",
    "# Note: max 100 results specified via aplimit, apnamespace=1 specifies that we want Talk pages only\n",
    "url = api_url + '?action=query&list=allpages&apnamespace=1&apprefix='+ page +'/Archive&aplimit=100'\n",
    "print('Requesting', url) \n",
    "print('Click link above for human-readable view of json')\n",
    "\n",
    "# request url\n",
    "response = requests.get(url + '&format=json', headers=headers)\n",
    "\n",
    "# decode the json received\n",
    "data = response.json()\n",
    "\n",
    "# loop through the list of pages retrieved and add them to the talk_pages queue\n",
    "for archive_page in data['query']['allpages']:\n",
    "    print('Add page:', archive_page['title'])\n",
    "    talk_pages.append(archive_page['title'].replace(' ','_'))\n",
    "    \n",
    "# loop through talk_pages queue\n",
    "for talk_page in talk_pages:\n",
    "    # create url for API call to get markup of talk page\n",
    "    url = api_url + '?action=parse&prop=text&page=' + talk_page\n",
    "    print('Requesting', url) \n",
    "    \n",
    "    # request url\n",
    "    response = requests.get(url + '&format=json', headers=headers)\n",
    "\n",
    "    # decode the json received\n",
    "    data = response.json()\n",
    "    \n",
    "    # convert url to filename\n",
    "    filename = url_to_filename(talk_page) + '.txt'\n",
    "            \n",
    "    #convert html from the api to txt\n",
    "    txt = wiki_html_to_txt(data['parse']['text']['*'])\n",
    "            \n",
    "    #save txt to file ...\n",
    "    with open(corpus_path + filename, 'w', encoding='utf8') as f:\n",
    "        f.write(txt)\n",
    "            \n",
    "    # rest before another request\n",
    "    time.sleep(sleep_seconds)\n",
    "    \n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip your corpus\n",
    "\n",
    "The next cell zips your corpus so you can download it and use it in software like AntConc. Tick the checkbox next to the .zip file and click the Download button to save a copy to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_filename = os.path.dirname(corpus_path) + '.zip'\n",
    "\n",
    "zipf = zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED)\n",
    "\n",
    "# ziph is zipfile handle\n",
    "for root, dirs, files in os.walk(corpus_path):\n",
    "    for file in files:\n",
    "        zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(corpus_path, '..')))\n",
    "      \n",
    "zipf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do next?\n",
    "\n",
    "### Try a little analysis\n",
    "\n",
    "Load your corpus in AntConc. Run a Keyword comparison against the BNC to derive lists of over-represented words. Can you group these? Some will be related to features of a Wikipedia Talk page and the editors who are leaving comments. Other words will be related to the topics of discussion within the Talk pages.\n",
    "\n",
    "### A better corpus?\n",
    "\n",
    "We could probably improve this corpus. There is some cleaning being done when the Wikipedia HTML is converted to text. More cleaning is possible. Choices about what to leave in and remove (or \"clean\") could be important to our analysis.\n",
    "\n",
    "Another way we could improve our corpus would be to segment the Talk pages. Currently we are collecting each Talk page and related archived Talk pages and saving each page as a separate file. However, Talk pages represent different topics of discussion and specific comments by users. This is beyond the scope of this lab, but splitting up the Talk pages by topic sections or by user comments could be useful for specific kinds of research. Why might this be useful?\n",
    "\n",
    "### Other APIs\n",
    "\n",
    "What other APIs are there for collecting texts? Search the web and see what you can find. You will find code others have written to build data-sets of texts on Github, Kaggle and elsewhere.\n",
    "\n",
    "### Collect another corpus\n",
    "\n",
    "After you have collected a Talk:COVID-19 corpus scope Wikipedia for another Talk page to collect. Look for an article that has a few archived pages of talk (10-20 pages is ideal).\n",
    "\n",
    "When you have decided on a page go back to the Settings cell and change the page and corpus_path and collect the other corpus. \n",
    "\n",
    "#### Ideas:\n",
    "1. You could collect the Talk pages for articles that address pandemic response in specific countries:  \n",
    "https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_Kingdom  \n",
    "https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States  \n",
    "You could compare these with the first corpus we created.\n",
    "\n",
    "2. You can browse Wikipedia for controversial pages here: \n",
    "https://en.wikipedia.org/wiki/Wikipedia:List_of_controversial_issues\n",
    "\n",
    "3. What interests you? What do you think might be controversial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
